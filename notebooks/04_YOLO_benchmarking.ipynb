{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dae7bd7",
   "metadata": {},
   "source": [
    "# YOLO Benchmarking Walkthrough\n",
    "\n",
    "This guide walks through the complete process of running YOLO benchmarks on various datasets, from initial setup to result analysis and comparison.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Initial Setup and Import](#initial-setup-and-import)\n",
    "2. [Exploring Available Models and Datasets](#exploring-available-models-and-datasets)\n",
    "3. [Selecting Models and Datasets](#selecting-models-and-datasets)\n",
    "4. [Running Individual Benchmarks](#running-individual-benchmarks)\n",
    "5. [Running Multiple Benchmarks](#running-multiple-benchmarks)\n",
    "6. [Combining and Analyzing Results](#combining-and-analyzing-results)\n",
    "7. [Comparing Model Performance](#comparing-model-performance)\n",
    "8. [Advanced Analysis and Visualization](#advanced-analysis-and-visualization)\n",
    "\n",
    "## Initial Setup and Import\n",
    "\n",
    "First, ensure you have the required dependencies installed and are in the correct directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1786ad1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Successfully imported YOLOBenchmark\n",
      "\n",
      "Initializing benchmark...\n"
     ]
    }
   ],
   "source": [
    "# Activate virtual environment if using one\n",
    "# source venv/bin/activate  # On Unix/Mac\n",
    "# venv\\Scripts\\activate     # On Windows\n",
    "\n",
    "# Import the benchmark class\n",
    "from beeyolo.benchmark_YOLO import YOLOBenchmark\n",
    "\n",
    "# Initialize the benchmark\n",
    "benchmark = YOLOBenchmark(\n",
    "    models_dir=\"../models\",\n",
    "    datasets_dir=\"../datasets\", \n",
    "    benchmarks_dir=\"../benchmarks\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Successfully imported YOLOBenchmark\")\n",
    "print(\"\\nInitializing benchmark...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb583e11",
   "metadata": {},
   "source": [
    "## Exploring Available Models and Datasets\n",
    "\n",
    "Before running benchmarks, let's see what's available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdaa927a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available models:\n",
      "  1. mortalityYOLO\n",
      "  2. beeYOLO\n",
      "  3. insectYOLO\n",
      "\n",
      "Available datasets:\n",
      "  1. ivybee-data\n",
      "  2. bee-feeder-data\n",
      "  3. insect-data\n",
      "  4. alive-dead-data\n",
      "  5. bumblebee-data\n",
      "  6. hoverfly-data\n",
      "  7. honeybee-data\n",
      "\n",
      "Detailed Information:\n",
      "Models:\n",
      "  mortalityYOLO: 6.0 MB\n",
      "  beeYOLO: 5.9 MB\n",
      "  insectYOLO: 17.6 MB\n",
      "Datasets:\n",
      "  ivybee-data: 2 classes, 38 validation images\n",
      "  bee-feeder-data: 2 classes, 30 validation images\n",
      "  insect-data: 2 classes, 166 validation images\n",
      "  alive-dead-data: 3 classes, 50 validation images\n",
      "  bumblebee-data: 2 classes, 50 validation images\n",
      "  hoverfly-data: 2 classes, 38 validation images\n",
      "  honeybee-data: 2 classes, 40 validation images\n"
     ]
    }
   ],
   "source": [
    "# Get available models\n",
    "models = benchmark.get_available_models()\n",
    "print(\"\\nAvailable models:\")\n",
    "for i, model in enumerate(models, 1):\n",
    "    print(f\"  {i}. {model}\")\n",
    "\n",
    "# Get available datasets\n",
    "datasets = benchmark.get_available_datasets()\n",
    "print(\"\\nAvailable datasets:\")\n",
    "for i, dataset in enumerate(datasets, 1):\n",
    "    print(f\"  {i}. {dataset}\")\n",
    "\n",
    "# Get detailed information about models and datasets (cleaner approach)\n",
    "print(\"\\nDetailed Information:\")\n",
    "print(\"Models:\")\n",
    "for model_name in models:\n",
    "    model_path = benchmark.models_dir / f\"{model_name}.pt\"\n",
    "    if model_path.exists():\n",
    "        file_size = model_path.stat().st_size / (1024 * 1024)  # MB\n",
    "        print(f\"  {model_name}: {file_size:.1f} MB\")\n",
    "\n",
    "print(\"Datasets:\")\n",
    "for dataset_name in datasets:\n",
    "    try:\n",
    "        # Get basic dataset info without verbose logging\n",
    "        dataset_path = benchmark.datasets_dir / dataset_name\n",
    "        if (dataset_path / \"data.yaml\").exists():\n",
    "            # Count validation images directly\n",
    "            val_images_dir = dataset_path / \"val\" / \"images\"\n",
    "            val_labels_dir = dataset_path / \"val\" / \"labels\"\n",
    "            \n",
    "            num_val_images = len(list(val_images_dir.glob(\"*.png\"))) if val_images_dir.exists() else 0\n",
    "            num_val_labels = len(list(val_labels_dir.glob(\"*.txt\"))) if val_labels_dir.exists() else 0\n",
    "            \n",
    "            # Get class count from data.yaml\n",
    "            import yaml\n",
    "            with open(dataset_path / \"data.yaml\", 'r') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "                num_classes = config.get('nc', 0)\n",
    "            \n",
    "            print(f\"  {dataset_name}: {num_classes} classes, {num_val_images} validation images\")\n",
    "        else:\n",
    "            print(f\"  {dataset_name}: No data.yaml found\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {dataset_name}: Error getting info - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5660eca9",
   "metadata": {},
   "source": [
    "## Selecting Models and Datasets\n",
    "\n",
    "Choose which models and datasets to benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55fd4581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING MODEL: insectYOLO\n",
      "============================================================\n",
      "âœ“ Model selected: insectYOLO\n",
      "\n",
      "============================================================\n",
      "Running benchmark on bumblebee-data\n",
      "Data mode: val\n",
      "============================================================\n",
      "Evaluating model ../models/insectYOLO.pt on dataset bumblebee-data (val mode)\n",
      "DEBUG: evaluate_model_on_dataset called with dataset_name='bumblebee-data'\n",
      "âœ“ Model loaded: ../models/insectYOLO.pt\n",
      "Model class names: {0: 'bee', 1: 'feeder'}\n",
      "DEBUG: About to call load_dataset_info with dataset_name='bumblebee-data'\n",
      "    load_dataset_info called with dataset_name: 'bumblebee-data'\n",
      "    Looking for yaml at: ../datasets/bumblebee-data/data.yaml\n",
      "Dataset: bumblebee-data\n",
      "Classes: ['bee', 'feeder']\n",
      "Mode: val\n",
      "  Creating temp data.yaml for dataset: 'bumblebee-data', mode: 'val'\n",
      "DEBUG: About to call _create_temp_data_yaml with dataset_name='bumblebee-data'\n",
      "    _create_temp_data_yaml called with dataset_name: 'bumblebee-data', mode: 'val'\n",
      "    DEBUG: _create_temp_data_yaml received dataset_name='bumblebee-data'\n",
      "  Creating temporary data.yaml at: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmpzgoi8j8x/temp_data.yaml\n",
      "  Loading dataset config for: bumblebee-data\n",
      "  Dataset path: ../datasets/bumblebee-data\n",
      "  DEBUG: About to call load_dataset_info with dataset_name='bumblebee-data'\n",
      "  DEBUG: dataset_name value is still 'bumblebee-data'\n",
      "    load_dataset_info called with dataset_name: 'bumblebee-data'\n",
      "    Looking for yaml at: ../datasets/bumblebee-data/data.yaml\n",
      "  âœ“ Dataset config loaded successfully\n",
      "Running Ultralytics validation...\n",
      "  Validation results will be saved to: ../benchmarks/temp_validation\n",
      "Ultralytics 8.3.179 ðŸš€ Python-3.13.5 torch-2.8.0 CPU (Apple M2 Pro)\n",
      "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.2Â±0.1 ms, read: 370.7Â±38.1 MB/s, size: 377.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/user/Documents/BEEhaviourLab/BEEhaviourLab-YOLO-training/datasets/bumblebee-data/val/labels.cache... 49 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:15<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50        200       0.98       0.98      0.982      0.848\n",
      "Speed: 2.0ms preprocess, 270.8ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1m../benchmarks/temp_validation/bumblebee-data_val\u001b[0m\n",
      "âœ“ Validation completed successfully\n",
      "Available Ultralytics metrics: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)', 'fitness']\n",
      "Sample metrics: {'metrics/precision(B)': np.float64(0.9795827688085168), 'metrics/recall(B)': np.float64(0.98), 'metrics/mAP50(B)': np.float64(0.9817751340271206), 'metrics/mAP50-95(B)': np.float64(0.8475792438635674), 'fitness': np.float64(0.8609988328799227)}\n",
      "    load_dataset_info called with dataset_name: 'bumblebee-data'\n",
      "    Looking for yaml at: ../datasets/bumblebee-data/data.yaml\n",
      "  âœ“ Benchmark evaluation completed successfully\n",
      "  Returning results for bumblebee-data\n",
      "  Cleaning up temporary file: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmpzgoi8j8x/temp_data.yaml\n",
      "  Cleaning up temporary directory: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmpzgoi8j8x\n",
      "  âœ“ Temporary files cleaned up successfully\n",
      "  Cleaning up temp_validation directory: ../benchmarks/temp_validation\n",
      "  âœ“ Temp validation directory cleaned up successfully\n",
      "  Storing benchmark results...\n",
      "  âœ“ Results stored with key: insectYOLO_bumblebee-data_val\n",
      "  Saving results for bumblebee-data...\n",
      "    Saving results directly to benchmarks folder\n",
      "    Saving JSON results to: ../benchmarks/benchmark_results_bumblebee-data_20250818_165614.json\n",
      "âœ“ Results saved to ../benchmarks/benchmark_results_bumblebee-data_20250818_165614.json\n",
      "  âœ“ Results saved for bumblebee-data\n",
      "\n",
      "============================================================\n",
      "Running benchmark on honeybee-data\n",
      "Data mode: val\n",
      "============================================================\n",
      "Evaluating model ../models/insectYOLO.pt on dataset honeybee-data (val mode)\n",
      "DEBUG: evaluate_model_on_dataset called with dataset_name='honeybee-data'\n",
      "âœ“ Model loaded: ../models/insectYOLO.pt\n",
      "Model class names: {0: 'bee', 1: 'feeder'}\n",
      "DEBUG: About to call load_dataset_info with dataset_name='honeybee-data'\n",
      "    load_dataset_info called with dataset_name: 'honeybee-data'\n",
      "    Looking for yaml at: ../datasets/honeybee-data/data.yaml\n",
      "Dataset: honeybee-data\n",
      "Classes: ['bee', 'feeder']\n",
      "Mode: val\n",
      "  Creating temp data.yaml for dataset: 'honeybee-data', mode: 'val'\n",
      "DEBUG: About to call _create_temp_data_yaml with dataset_name='honeybee-data'\n",
      "    _create_temp_data_yaml called with dataset_name: 'honeybee-data', mode: 'val'\n",
      "    DEBUG: _create_temp_data_yaml received dataset_name='honeybee-data'\n",
      "  Creating temporary data.yaml at: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmpt9rjp7rg/temp_data.yaml\n",
      "  Loading dataset config for: honeybee-data\n",
      "  Dataset path: ../datasets/honeybee-data\n",
      "  DEBUG: About to call load_dataset_info with dataset_name='honeybee-data'\n",
      "  DEBUG: dataset_name value is still 'honeybee-data'\n",
      "    load_dataset_info called with dataset_name: 'honeybee-data'\n",
      "    Looking for yaml at: ../datasets/honeybee-data/data.yaml\n",
      "  âœ“ Dataset config loaded successfully\n",
      "Running Ultralytics validation...\n",
      "  Validation results will be saved to: ../benchmarks/temp_validation\n",
      "Ultralytics 8.3.179 ðŸš€ Python-3.13.5 torch-2.8.0 CPU (Apple M2 Pro)\n",
      "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 658.0Â±206.2 MB/s, size: 812.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/user/Documents/BEEhaviourLab/BEEhaviourLab-YOLO-training/datasets/honeybee-data/val/labels.cache... 40 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40         80          1          1      0.995      0.842\n",
      "Speed: 1.5ms preprocess, 211.3ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1m../benchmarks/temp_validation/honeybee-data_val\u001b[0m\n",
      "âœ“ Validation completed successfully\n",
      "Available Ultralytics metrics: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)', 'fitness']\n",
      "Sample metrics: {'metrics/precision(B)': np.float64(1.0), 'metrics/recall(B)': np.float64(1.0), 'metrics/mAP50(B)': np.float64(0.995), 'metrics/mAP50-95(B)': np.float64(0.8416463194380779), 'fitness': np.float64(0.8569816874942702)}\n",
      "    load_dataset_info called with dataset_name: 'honeybee-data'\n",
      "    Looking for yaml at: ../datasets/honeybee-data/data.yaml\n",
      "  âœ“ Benchmark evaluation completed successfully\n",
      "  Returning results for honeybee-data\n",
      "  Cleaning up temporary file: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmpt9rjp7rg/temp_data.yaml\n",
      "  Cleaning up temporary directory: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmpt9rjp7rg\n",
      "  âœ“ Temporary files cleaned up successfully\n",
      "  Cleaning up temp_validation directory: ../benchmarks/temp_validation\n",
      "  âœ“ Temp validation directory cleaned up successfully\n",
      "  Storing benchmark results...\n",
      "  âœ“ Results stored with key: insectYOLO_honeybee-data_val\n",
      "  Saving results for honeybee-data...\n",
      "    Saving results directly to benchmarks folder\n",
      "    Saving JSON results to: ../benchmarks/benchmark_results_honeybee-data_20250818_165627.json\n",
      "âœ“ Results saved to ../benchmarks/benchmark_results_honeybee-data_20250818_165627.json\n",
      "  âœ“ Results saved for honeybee-data\n",
      "\n",
      "============================================================\n",
      "Running benchmark on ivybee-data\n",
      "Data mode: val\n",
      "============================================================\n",
      "Evaluating model ../models/insectYOLO.pt on dataset ivybee-data (val mode)\n",
      "DEBUG: evaluate_model_on_dataset called with dataset_name='ivybee-data'\n",
      "âœ“ Model loaded: ../models/insectYOLO.pt\n",
      "Model class names: {0: 'bee', 1: 'feeder'}\n",
      "DEBUG: About to call load_dataset_info with dataset_name='ivybee-data'\n",
      "    load_dataset_info called with dataset_name: 'ivybee-data'\n",
      "    Looking for yaml at: ../datasets/ivybee-data/data.yaml\n",
      "Dataset: ivybee-data\n",
      "Classes: ['bee', 'feeder']\n",
      "Mode: val\n",
      "  Creating temp data.yaml for dataset: 'ivybee-data', mode: 'val'\n",
      "DEBUG: About to call _create_temp_data_yaml with dataset_name='ivybee-data'\n",
      "    _create_temp_data_yaml called with dataset_name: 'ivybee-data', mode: 'val'\n",
      "    DEBUG: _create_temp_data_yaml received dataset_name='ivybee-data'\n",
      "  Creating temporary data.yaml at: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmpi3sxb_el/temp_data.yaml\n",
      "  Loading dataset config for: ivybee-data\n",
      "  Dataset path: ../datasets/ivybee-data\n",
      "  DEBUG: About to call load_dataset_info with dataset_name='ivybee-data'\n",
      "  DEBUG: dataset_name value is still 'ivybee-data'\n",
      "    load_dataset_info called with dataset_name: 'ivybee-data'\n",
      "    Looking for yaml at: ../datasets/ivybee-data/data.yaml\n",
      "  âœ“ Dataset config loaded successfully\n",
      "Running Ultralytics validation...\n",
      "  Validation results will be saved to: ../benchmarks/temp_validation\n",
      "Ultralytics 8.3.179 ðŸš€ Python-3.13.5 torch-2.8.0 CPU (Apple M2 Pro)\n",
      "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 568.1Â±119.6 MB/s, size: 757.2 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/user/Documents/BEEhaviourLab/BEEhaviourLab-YOLO-training/datasets/ivybee-data/val/labels.cache... 38 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38         76      0.987      0.987      0.989      0.845\n",
      "Speed: 1.3ms preprocess, 182.3ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1m../benchmarks/temp_validation/ivybee-data_val\u001b[0m\n",
      "âœ“ Validation completed successfully\n",
      "Available Ultralytics metrics: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)', 'fitness']\n",
      "Sample metrics: {'metrics/precision(B)': np.float64(0.986842105263158), 'metrics/recall(B)': np.float64(0.986842105263158), 'metrics/mAP50(B)': np.float64(0.9894973684210526), 'metrics/mAP50-95(B)': np.float64(0.8452112381591702), 'fitness': np.float64(0.8596398511853585)}\n",
      "    load_dataset_info called with dataset_name: 'ivybee-data'\n",
      "    Looking for yaml at: ../datasets/ivybee-data/data.yaml\n",
      "  âœ“ Benchmark evaluation completed successfully\n",
      "  Returning results for ivybee-data\n",
      "  Cleaning up temporary file: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmpi3sxb_el/temp_data.yaml\n",
      "  Cleaning up temporary directory: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmpi3sxb_el\n",
      "  âœ“ Temporary files cleaned up successfully\n",
      "  Cleaning up temp_validation directory: ../benchmarks/temp_validation\n",
      "  âœ“ Temp validation directory cleaned up successfully\n",
      "  Storing benchmark results...\n",
      "  âœ“ Results stored with key: insectYOLO_ivybee-data_val\n",
      "  Saving results for ivybee-data...\n",
      "    Saving results directly to benchmarks folder\n",
      "    Saving JSON results to: ../benchmarks/benchmark_results_ivybee-data_20250818_165639.json\n",
      "âœ“ Results saved to ../benchmarks/benchmark_results_ivybee-data_20250818_165639.json\n",
      "  âœ“ Results saved for ivybee-data\n",
      "\n",
      "============================================================\n",
      "Running benchmark on hoverfly-data\n",
      "Data mode: val\n",
      "============================================================\n",
      "Evaluating model ../models/insectYOLO.pt on dataset hoverfly-data (val mode)\n",
      "DEBUG: evaluate_model_on_dataset called with dataset_name='hoverfly-data'\n",
      "âœ“ Model loaded: ../models/insectYOLO.pt\n",
      "Model class names: {0: 'bee', 1: 'feeder'}\n",
      "DEBUG: About to call load_dataset_info with dataset_name='hoverfly-data'\n",
      "    load_dataset_info called with dataset_name: 'hoverfly-data'\n",
      "    Looking for yaml at: ../datasets/hoverfly-data/data.yaml\n",
      "Dataset: hoverfly-data\n",
      "Classes: ['bee', 'feeder']\n",
      "Mode: val\n",
      "  Creating temp data.yaml for dataset: 'hoverfly-data', mode: 'val'\n",
      "DEBUG: About to call _create_temp_data_yaml with dataset_name='hoverfly-data'\n",
      "    _create_temp_data_yaml called with dataset_name: 'hoverfly-data', mode: 'val'\n",
      "    DEBUG: _create_temp_data_yaml received dataset_name='hoverfly-data'\n",
      "  Creating temporary data.yaml at: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmppsyzojdr/temp_data.yaml\n",
      "  Loading dataset config for: hoverfly-data\n",
      "  Dataset path: ../datasets/hoverfly-data\n",
      "  DEBUG: About to call load_dataset_info with dataset_name='hoverfly-data'\n",
      "  DEBUG: dataset_name value is still 'hoverfly-data'\n",
      "    load_dataset_info called with dataset_name: 'hoverfly-data'\n",
      "    Looking for yaml at: ../datasets/hoverfly-data/data.yaml\n",
      "  âœ“ Dataset config loaded successfully\n",
      "Running Ultralytics validation...\n",
      "  Validation results will be saved to: ../benchmarks/temp_validation\n",
      "Ultralytics 8.3.179 ðŸš€ Python-3.13.5 torch-2.8.0 CPU (Apple M2 Pro)\n",
      "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 603.1Â±187.8 MB/s, size: 849.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/user/Documents/BEEhaviourLab/BEEhaviourLab-YOLO-training/datasets/hoverfly-data/val/labels.cache... 38 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:10<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38         76          1      0.987      0.991       0.64\n",
      "Speed: 2.0ms preprocess, 221.4ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1m../benchmarks/temp_validation/hoverfly-data_val\u001b[0m\n",
      "âœ“ Validation completed successfully\n",
      "Available Ultralytics metrics: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)', 'fitness']\n",
      "Sample metrics: {'metrics/precision(B)': np.float64(1.0), 'metrics/recall(B)': np.float64(0.986842105263158), 'metrics/mAP50(B)': np.float64(0.9907), 'metrics/mAP50-95(B)': np.float64(0.6403265979651396), 'fitness': np.float64(0.6753639381686256)}\n",
      "    load_dataset_info called with dataset_name: 'hoverfly-data'\n",
      "    Looking for yaml at: ../datasets/hoverfly-data/data.yaml\n",
      "  âœ“ Benchmark evaluation completed successfully\n",
      "  Returning results for hoverfly-data\n",
      "  Cleaning up temporary file: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmppsyzojdr/temp_data.yaml\n",
      "  Cleaning up temporary directory: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmppsyzojdr\n",
      "  âœ“ Temporary files cleaned up successfully\n",
      "  Cleaning up temp_validation directory: ../benchmarks/temp_validation\n",
      "  âœ“ Temp validation directory cleaned up successfully\n",
      "  Storing benchmark results...\n",
      "  âœ“ Results stored with key: insectYOLO_hoverfly-data_val\n",
      "  Saving results for hoverfly-data...\n",
      "    Saving results directly to benchmarks folder\n",
      "    Saving JSON results to: ../benchmarks/benchmark_results_hoverfly-data_20250818_165651.json\n",
      "âœ“ Results saved to ../benchmarks/benchmark_results_hoverfly-data_20250818_165651.json\n",
      "  âœ“ Results saved for hoverfly-data\n",
      "\n",
      "============================================================\n",
      "Benchmark Results Summary:\n",
      "============================================================\n",
      "Total datasets processed: 4\n",
      "  bumblebee-data: 0.980 precision, 0.980 recall\n",
      "  honeybee-data: 1.000 precision, 1.000 recall\n",
      "  ivybee-data: 0.987 precision, 0.987 recall\n",
      "  hoverfly-data: 1.000 precision, 0.987 recall\n",
      "\n",
      "============================================================\n",
      "TESTING MODEL: beeYOLO\n",
      "============================================================\n",
      "âœ“ Model selected: beeYOLO\n",
      "\n",
      "============================================================\n",
      "Running benchmark on bumblebee-data\n",
      "Data mode: val\n",
      "============================================================\n",
      "Evaluating model ../models/beeYOLO.pt on dataset bumblebee-data (val mode)\n",
      "DEBUG: evaluate_model_on_dataset called with dataset_name='bumblebee-data'\n",
      "âœ“ Model loaded: ../models/beeYOLO.pt\n",
      "Model class names: {0: 'bee', 1: 'feeder'}\n",
      "DEBUG: About to call load_dataset_info with dataset_name='bumblebee-data'\n",
      "    load_dataset_info called with dataset_name: 'bumblebee-data'\n",
      "    Looking for yaml at: ../datasets/bumblebee-data/data.yaml\n",
      "Dataset: bumblebee-data\n",
      "Classes: ['bee', 'feeder']\n",
      "Mode: val\n",
      "  Creating temp data.yaml for dataset: 'bumblebee-data', mode: 'val'\n",
      "DEBUG: About to call _create_temp_data_yaml with dataset_name='bumblebee-data'\n",
      "    _create_temp_data_yaml called with dataset_name: 'bumblebee-data', mode: 'val'\n",
      "    DEBUG: _create_temp_data_yaml received dataset_name='bumblebee-data'\n",
      "  Creating temporary data.yaml at: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmp80w6lmyw/temp_data.yaml\n",
      "  Loading dataset config for: bumblebee-data\n",
      "  Dataset path: ../datasets/bumblebee-data\n",
      "  DEBUG: About to call load_dataset_info with dataset_name='bumblebee-data'\n",
      "  DEBUG: dataset_name value is still 'bumblebee-data'\n",
      "    load_dataset_info called with dataset_name: 'bumblebee-data'\n",
      "    Looking for yaml at: ../datasets/bumblebee-data/data.yaml\n",
      "  âœ“ Dataset config loaded successfully\n",
      "Running Ultralytics validation...\n",
      "  Validation results will be saved to: ../benchmarks/temp_validation\n",
      "Ultralytics 8.3.179 ðŸš€ Python-3.13.5 torch-2.8.0 CPU (Apple M2 Pro)\n",
      "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 437.8Â±119.5 MB/s, size: 393.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/user/Documents/BEEhaviourLab/BEEhaviourLab-YOLO-training/datasets/bumblebee-data/val/labels.cache... 49 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:11<00:00,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50        200       0.98       0.97       0.98      0.769\n",
      "Speed: 1.0ms preprocess, 207.1ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1m../benchmarks/temp_validation/bumblebee-data_val\u001b[0m\n",
      "âœ“ Validation completed successfully\n",
      "Available Ultralytics metrics: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)', 'fitness']\n",
      "Sample metrics: {'metrics/precision(B)': np.float64(0.9797959183673469), 'metrics/recall(B)': np.float64(0.97), 'metrics/mAP50(B)': np.float64(0.9796982928693582), 'metrics/mAP50-95(B)': np.float64(0.7687158649910112), 'fitness': np.float64(0.7898141077788459)}\n",
      "    load_dataset_info called with dataset_name: 'bumblebee-data'\n",
      "    Looking for yaml at: ../datasets/bumblebee-data/data.yaml\n",
      "  âœ“ Benchmark evaluation completed successfully\n",
      "  Returning results for bumblebee-data\n",
      "  Cleaning up temporary file: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmp80w6lmyw/temp_data.yaml\n",
      "  Cleaning up temporary directory: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmp80w6lmyw\n",
      "  âœ“ Temporary files cleaned up successfully\n",
      "  Cleaning up temp_validation directory: ../benchmarks/temp_validation\n",
      "  âœ“ Temp validation directory cleaned up successfully\n",
      "  Storing benchmark results...\n",
      "  âœ“ Results stored with key: beeYOLO_bumblebee-data_val\n",
      "  Saving results for bumblebee-data...\n",
      "    Saving results directly to benchmarks folder\n",
      "    Saving JSON results to: ../benchmarks/benchmark_results_bumblebee-data_20250818_165705.json\n",
      "âœ“ Results saved to ../benchmarks/benchmark_results_bumblebee-data_20250818_165705.json\n",
      "  âœ“ Results saved for bumblebee-data\n",
      "\n",
      "============================================================\n",
      "Running benchmark on honeybee-data\n",
      "Data mode: val\n",
      "============================================================\n",
      "Evaluating model ../models/beeYOLO.pt on dataset honeybee-data (val mode)\n",
      "DEBUG: evaluate_model_on_dataset called with dataset_name='honeybee-data'\n",
      "âœ“ Model loaded: ../models/beeYOLO.pt\n",
      "Model class names: {0: 'bee', 1: 'feeder'}\n",
      "DEBUG: About to call load_dataset_info with dataset_name='honeybee-data'\n",
      "    load_dataset_info called with dataset_name: 'honeybee-data'\n",
      "    Looking for yaml at: ../datasets/honeybee-data/data.yaml\n",
      "Dataset: honeybee-data\n",
      "Classes: ['bee', 'feeder']\n",
      "Mode: val\n",
      "  Creating temp data.yaml for dataset: 'honeybee-data', mode: 'val'\n",
      "DEBUG: About to call _create_temp_data_yaml with dataset_name='honeybee-data'\n",
      "    _create_temp_data_yaml called with dataset_name: 'honeybee-data', mode: 'val'\n",
      "    DEBUG: _create_temp_data_yaml received dataset_name='honeybee-data'\n",
      "  Creating temporary data.yaml at: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmpr3umo1pc/temp_data.yaml\n",
      "  Loading dataset config for: honeybee-data\n",
      "  Dataset path: ../datasets/honeybee-data\n",
      "  DEBUG: About to call load_dataset_info with dataset_name='honeybee-data'\n",
      "  DEBUG: dataset_name value is still 'honeybee-data'\n",
      "    load_dataset_info called with dataset_name: 'honeybee-data'\n",
      "    Looking for yaml at: ../datasets/honeybee-data/data.yaml\n",
      "  âœ“ Dataset config loaded successfully\n",
      "Running Ultralytics validation...\n",
      "  Validation results will be saved to: ../benchmarks/temp_validation\n",
      "Ultralytics 8.3.179 ðŸš€ Python-3.13.5 torch-2.8.0 CPU (Apple M2 Pro)\n",
      "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 562.9Â±156.3 MB/s, size: 789.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/user/Documents/BEEhaviourLab/BEEhaviourLab-YOLO-training/datasets/honeybee-data/val/labels.cache... 40 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40         80      0.762      0.666      0.741       0.39\n",
      "Speed: 1.9ms preprocess, 196.3ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m../benchmarks/temp_validation/honeybee-data_val\u001b[0m\n",
      "âœ“ Validation completed successfully\n",
      "Available Ultralytics metrics: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)', 'fitness']\n",
      "Sample metrics: {'metrics/precision(B)': np.float64(0.7616612114796347), 'metrics/recall(B)': np.float64(0.6662291748766561), 'metrics/mAP50(B)': np.float64(0.7407731232844016), 'metrics/mAP50-95(B)': np.float64(0.38974540092158444), 'fitness': np.float64(0.42484817315786616)}\n",
      "    load_dataset_info called with dataset_name: 'honeybee-data'\n",
      "    Looking for yaml at: ../datasets/honeybee-data/data.yaml\n",
      "  âœ“ Benchmark evaluation completed successfully\n",
      "  Returning results for honeybee-data\n",
      "  Cleaning up temporary file: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmpr3umo1pc/temp_data.yaml\n",
      "  Cleaning up temporary directory: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmpr3umo1pc\n",
      "  âœ“ Temporary files cleaned up successfully\n",
      "  Cleaning up temp_validation directory: ../benchmarks/temp_validation\n",
      "  âœ“ Temp validation directory cleaned up successfully\n",
      "  Storing benchmark results...\n",
      "  âœ“ Results stored with key: beeYOLO_honeybee-data_val\n",
      "  Saving results for honeybee-data...\n",
      "    Saving results directly to benchmarks folder\n",
      "    Saving JSON results to: ../benchmarks/benchmark_results_honeybee-data_20250818_165717.json\n",
      "âœ“ Results saved to ../benchmarks/benchmark_results_honeybee-data_20250818_165717.json\n",
      "  âœ“ Results saved for honeybee-data\n",
      "\n",
      "============================================================\n",
      "Running benchmark on ivybee-data\n",
      "Data mode: val\n",
      "============================================================\n",
      "Evaluating model ../models/beeYOLO.pt on dataset ivybee-data (val mode)\n",
      "DEBUG: evaluate_model_on_dataset called with dataset_name='ivybee-data'\n",
      "âœ“ Model loaded: ../models/beeYOLO.pt\n",
      "Model class names: {0: 'bee', 1: 'feeder'}\n",
      "DEBUG: About to call load_dataset_info with dataset_name='ivybee-data'\n",
      "    load_dataset_info called with dataset_name: 'ivybee-data'\n",
      "    Looking for yaml at: ../datasets/ivybee-data/data.yaml\n",
      "Dataset: ivybee-data\n",
      "Classes: ['bee', 'feeder']\n",
      "Mode: val\n",
      "  Creating temp data.yaml for dataset: 'ivybee-data', mode: 'val'\n",
      "DEBUG: About to call _create_temp_data_yaml with dataset_name='ivybee-data'\n",
      "    _create_temp_data_yaml called with dataset_name: 'ivybee-data', mode: 'val'\n",
      "    DEBUG: _create_temp_data_yaml received dataset_name='ivybee-data'\n",
      "  Creating temporary data.yaml at: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmpp723ux0d/temp_data.yaml\n",
      "  Loading dataset config for: ivybee-data\n",
      "  Dataset path: ../datasets/ivybee-data\n",
      "  DEBUG: About to call load_dataset_info with dataset_name='ivybee-data'\n",
      "  DEBUG: dataset_name value is still 'ivybee-data'\n",
      "    load_dataset_info called with dataset_name: 'ivybee-data'\n",
      "    Looking for yaml at: ../datasets/ivybee-data/data.yaml\n",
      "  âœ“ Dataset config loaded successfully\n",
      "Running Ultralytics validation...\n",
      "  Validation results will be saved to: ../benchmarks/temp_validation\n",
      "Ultralytics 8.3.179 ðŸš€ Python-3.13.5 torch-2.8.0 CPU (Apple M2 Pro)\n",
      "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 646.8Â±152.5 MB/s, size: 766.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/user/Documents/BEEhaviourLab/BEEhaviourLab-YOLO-training/datasets/ivybee-data/val/labels.cache... 38 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38         76        0.5      0.447      0.474      0.368\n",
      "Speed: 2.8ms preprocess, 212.2ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1m../benchmarks/temp_validation/ivybee-data_val\u001b[0m\n",
      "âœ“ Validation completed successfully\n",
      "Available Ultralytics metrics: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)', 'fitness']\n",
      "Sample metrics: {'metrics/precision(B)': np.float64(0.5), 'metrics/recall(B)': np.float64(0.4473684210526316), 'metrics/mAP50(B)': np.float64(0.4736250000000001), 'metrics/mAP50-95(B)': np.float64(0.3677767737104554), 'fitness': np.float64(0.3783615963394099)}\n",
      "    load_dataset_info called with dataset_name: 'ivybee-data'\n",
      "    Looking for yaml at: ../datasets/ivybee-data/data.yaml\n",
      "  âœ“ Benchmark evaluation completed successfully\n",
      "  Returning results for ivybee-data\n",
      "  Cleaning up temporary file: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmpp723ux0d/temp_data.yaml\n",
      "  Cleaning up temporary directory: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmpp723ux0d\n",
      "  âœ“ Temporary files cleaned up successfully\n",
      "  Cleaning up temp_validation directory: ../benchmarks/temp_validation\n",
      "  âœ“ Temp validation directory cleaned up successfully\n",
      "  Storing benchmark results...\n",
      "  âœ“ Results stored with key: beeYOLO_ivybee-data_val\n",
      "  Saving results for ivybee-data...\n",
      "    Saving results directly to benchmarks folder\n",
      "    Saving JSON results to: ../benchmarks/benchmark_results_ivybee-data_20250818_165729.json\n",
      "âœ“ Results saved to ../benchmarks/benchmark_results_ivybee-data_20250818_165729.json\n",
      "  âœ“ Results saved for ivybee-data\n",
      "\n",
      "============================================================\n",
      "Running benchmark on hoverfly-data\n",
      "Data mode: val\n",
      "============================================================\n",
      "Evaluating model ../models/beeYOLO.pt on dataset hoverfly-data (val mode)\n",
      "DEBUG: evaluate_model_on_dataset called with dataset_name='hoverfly-data'\n",
      "âœ“ Model loaded: ../models/beeYOLO.pt\n",
      "Model class names: {0: 'bee', 1: 'feeder'}\n",
      "DEBUG: About to call load_dataset_info with dataset_name='hoverfly-data'\n",
      "    load_dataset_info called with dataset_name: 'hoverfly-data'\n",
      "    Looking for yaml at: ../datasets/hoverfly-data/data.yaml\n",
      "Dataset: hoverfly-data\n",
      "Classes: ['bee', 'feeder']\n",
      "Mode: val\n",
      "  Creating temp data.yaml for dataset: 'hoverfly-data', mode: 'val'\n",
      "DEBUG: About to call _create_temp_data_yaml with dataset_name='hoverfly-data'\n",
      "    _create_temp_data_yaml called with dataset_name: 'hoverfly-data', mode: 'val'\n",
      "    DEBUG: _create_temp_data_yaml received dataset_name='hoverfly-data'\n",
      "  Creating temporary data.yaml at: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmpia1jdqr7/temp_data.yaml\n",
      "  Loading dataset config for: hoverfly-data\n",
      "  Dataset path: ../datasets/hoverfly-data\n",
      "  DEBUG: About to call load_dataset_info with dataset_name='hoverfly-data'\n",
      "  DEBUG: dataset_name value is still 'hoverfly-data'\n",
      "    load_dataset_info called with dataset_name: 'hoverfly-data'\n",
      "    Looking for yaml at: ../datasets/hoverfly-data/data.yaml\n",
      "  âœ“ Dataset config loaded successfully\n",
      "Running Ultralytics validation...\n",
      "  Validation results will be saved to: ../benchmarks/temp_validation\n",
      "Ultralytics 8.3.179 ðŸš€ Python-3.13.5 torch-2.8.0 CPU (Apple M2 Pro)\n",
      "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 676.6Â±221.6 MB/s, size: 830.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/user/Documents/BEEhaviourLab/BEEhaviourLab-YOLO-training/datasets/hoverfly-data/val/labels.cache... 38 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         38         76      0.833      0.671      0.747      0.357\n",
      "Speed: 1.0ms preprocess, 187.1ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1m../benchmarks/temp_validation/hoverfly-data_val\u001b[0m\n",
      "âœ“ Validation completed successfully\n",
      "Available Ultralytics metrics: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)', 'fitness']\n",
      "Sample metrics: {'metrics/precision(B)': np.float64(0.8333333333333333), 'metrics/recall(B)': np.float64(0.6710526315789473), 'metrics/mAP50(B)': np.float64(0.7466833333333334), 'metrics/mAP50-95(B)': np.float64(0.35745484648918474), 'fitness': np.float64(0.3963776951735996)}\n",
      "    load_dataset_info called with dataset_name: 'hoverfly-data'\n",
      "    Looking for yaml at: ../datasets/hoverfly-data/data.yaml\n",
      "  âœ“ Benchmark evaluation completed successfully\n",
      "  Returning results for hoverfly-data\n",
      "  Cleaning up temporary file: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmpia1jdqr7/temp_data.yaml\n",
      "  Cleaning up temporary directory: /var/folders/tq/xlt4lphs61q0t09n_9js3mbh0000gn/T/tmpia1jdqr7\n",
      "  âœ“ Temporary files cleaned up successfully\n",
      "  Cleaning up temp_validation directory: ../benchmarks/temp_validation\n",
      "  âœ“ Temp validation directory cleaned up successfully\n",
      "  Storing benchmark results...\n",
      "  âœ“ Results stored with key: beeYOLO_hoverfly-data_val\n",
      "  Saving results for hoverfly-data...\n",
      "    Saving results directly to benchmarks folder\n",
      "    Saving JSON results to: ../benchmarks/benchmark_results_hoverfly-data_20250818_165740.json\n",
      "âœ“ Results saved to ../benchmarks/benchmark_results_hoverfly-data_20250818_165740.json\n",
      "  âœ“ Results saved for hoverfly-data\n",
      "\n",
      "============================================================\n",
      "Benchmark Results Summary:\n",
      "============================================================\n",
      "Total datasets processed: 4\n",
      "  bumblebee-data: 0.980 precision, 0.970 recall\n",
      "  honeybee-data: 0.762 precision, 0.666 recall\n",
      "  ivybee-data: 0.500 precision, 0.447 recall\n",
      "  hoverfly-data: 0.833 precision, 0.671 recall\n",
      "\n",
      "âœ“ All model-dataset combinations completed!\n",
      "Total combinations tested: 8\n"
     ]
    }
   ],
   "source": [
    "# Select models and datasets\n",
    "selected_models = [\"insectYOLO\", \"beeYOLO\"]\n",
    "selected_datasets = [\"ivybee-data\", \"bumblebee-data\", \"honeybee-data\", \"hoverfly-data\"]\n",
    "\n",
    "# Run benchmarks for all combinations\n",
    "all_results = {}\n",
    "for model in selected_models:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TESTING MODEL: {model}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Set the current model\n",
    "    benchmark.select_model(model)\n",
    "    \n",
    "    # Run benchmarks on all selected datasets for this model\n",
    "    model_results = benchmark.run_all_benchmarks(\n",
    "        conf_threshold=0.5,\n",
    "        iou_threshold=0.5\n",
    "    )\n",
    "    \n",
    "    # Store results with model prefix\n",
    "    for dataset, result in model_results.items():\n",
    "        key = f\"{model}_{dataset}_val\"\n",
    "        all_results[key] = result\n",
    "\n",
    "print(f\"\\nâœ“ All model-dataset combinations completed!\")\n",
    "print(f\"Total combinations tested: {len(all_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621c5e81",
   "metadata": {},
   "source": [
    "## Combining and Analyzing Results\n",
    "\n",
    "Now combine all the JSON files in the benchmarks folder and analyze the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2466f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all benchmark results...\n",
      "âœ“ Loaded 8 benchmark result files\n",
      "\n",
      "Results DataFrame shape: (8, 14)\n",
      "\n",
      "First few rows:\n",
      "        Model         Dataset Mode  Total Images  Total Ground Truth  \\\n",
      "0     beeYOLO  bumblebee-data  val            50                 200   \n",
      "1  insectYOLO   honeybee-data  val            40                  80   \n",
      "2     beeYOLO     ivybee-data  val            38                  76   \n",
      "3  insectYOLO   hoverfly-data  val            38                  76   \n",
      "4     beeYOLO   hoverfly-data  val            38                  76   \n",
      "\n",
      "   Precision    Recall  F1-Score     mAP50  mAP50-95  Confidence Threshold  \\\n",
      "0   0.979796  0.970000  0.979698  0.979698  0.768716                   0.5   \n",
      "1   1.000000  1.000000  0.995000  0.995000  0.841646                   0.5   \n",
      "2   0.500000  0.447368  0.473625  0.473625  0.367777                   0.5   \n",
      "3   1.000000  0.986842  0.990700  0.990700  0.640327                   0.5   \n",
      "4   0.833333  0.671053  0.746683  0.746683  0.357455                   0.5   \n",
      "\n",
      "   IoU Threshold                                           Filename Timestamp  \n",
      "0            0.5  benchmark_results_bumblebee-data_20250818_1657...    165705  \n",
      "1            0.5  benchmark_results_honeybee-data_20250818_16562...    165627  \n",
      "2            0.5  benchmark_results_ivybee-data_20250818_165729....    165729  \n",
      "3            0.5  benchmark_results_hoverfly-data_20250818_16565...    165651  \n",
      "4            0.5  benchmark_results_hoverfly-data_20250818_16574...    165740  \n",
      "\n",
      "âœ“ Combined results saved to: ../benchmarks/combined_benchmark_results.json\n",
      "âœ“ Results DataFrame saved to: ../benchmarks/combined_benchmark_results.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def load_all_benchmark_results(benchmarks_dir):\n",
    "    \"\"\"Load all benchmark results from JSON files in the benchmarks directory.\"\"\"\n",
    "    results = []\n",
    "    benchmarks_path = Path(benchmarks_dir)\n",
    "    \n",
    "    # Find all JSON files\n",
    "    json_files = list(benchmarks_path.glob(\"benchmark_results_*.json\"))\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                # Add filename and timestamp for reference\n",
    "                data['filename'] = json_file.name\n",
    "                data['file_timestamp'] = json_file.stem.split('_')[-1]\n",
    "                results.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {json_file}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_results_dataframe(results):\n",
    "    \"\"\"Convert results to a pandas DataFrame for easy analysis.\"\"\"\n",
    "    df_data = []\n",
    "    \n",
    "    for result in results:\n",
    "        df_data.append({\n",
    "            'Model': Path(result['model_path']).stem,\n",
    "            'Dataset': result['dataset_name'],\n",
    "            'Mode': result['mode'],\n",
    "            'Total Images': result['total_images'],\n",
    "            'Total Ground Truth': result['total_ground_truth'],\n",
    "            'Precision': result['overall_precision'],\n",
    "            'Recall': result['overall_recall'],\n",
    "            'F1-Score': result['overall_f1_score'],\n",
    "            'mAP50': result['overall_map50'],\n",
    "            'mAP50-95': result['overall_map50_95'],\n",
    "            'Confidence Threshold': result['conf_threshold'],\n",
    "            'IoU Threshold': result['iou_threshold'],\n",
    "            'Filename': result['filename'],\n",
    "            'Timestamp': result['file_timestamp']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(df_data)\n",
    "\n",
    "# Load and combine all results\n",
    "print(\"Loading all benchmark results...\")\n",
    "all_results = load_all_benchmark_results(\"../benchmarks\")\n",
    "print(f\"âœ“ Loaded {len(all_results)} benchmark result files\")\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = create_results_dataframe(all_results)\n",
    "print(f\"\\nResults DataFrame shape: {results_df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(results_df.head())\n",
    "\n",
    "# Save combined results\n",
    "combined_file = \"../benchmarks/combined_benchmark_results.json\"\n",
    "with open(combined_file, 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "print(f\"\\nâœ“ Combined results saved to: {combined_file}\")\n",
    "\n",
    "# Save as CSV for easy analysis\n",
    "csv_file = \"../benchmarks/combined_benchmark_results.csv\"\n",
    "results_df.to_csv(csv_file, index=False)\n",
    "print(f\"âœ“ Results DataFrame saved to: {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6bbb20",
   "metadata": {},
   "source": [
    "## Comparing Model Performance\n",
    "\n",
    "Now analyze and compare the performance across different models and/or datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eecf0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "1. OVERALL PERFORMANCE BY MODEL:\n",
      "           Precision                Recall         F1-Score           mAP50  \\\n",
      "                mean     std count    mean     std     mean     std    mean   \n",
      "Model                                                                         \n",
      "beeYOLO       0.7687  0.2008     4  0.6887  0.2146   0.7352  0.2069  0.7352   \n",
      "insectYOLO    0.9916  0.0101     4  0.9884  0.0084   0.9892  0.0055  0.9892   \n",
      "\n",
      "                   mAP50-95          \n",
      "               std     mean     std  \n",
      "Model                                \n",
      "beeYOLO     0.2069   0.4709  0.1990  \n",
      "insectYOLO  0.0055   0.7937  0.1023  \n",
      "\n",
      "2. PERFORMANCE BY DATASET:\n",
      "               Precision                Recall         F1-Score          \\\n",
      "                    mean     std count    mean     std     mean     std   \n",
      "Dataset                                                                   \n",
      "bumblebee-data    0.9797  0.0002     2  0.9750  0.0071   0.9807  0.0015   \n",
      "honeybee-data     0.8808  0.1685     2  0.8331  0.2360   0.8679  0.1798   \n",
      "hoverfly-data     0.9167  0.1179     2  0.8289  0.2233   0.8687  0.1725   \n",
      "ivybee-data       0.7434  0.3442     2  0.7171  0.3815   0.7316  0.3648   \n",
      "\n",
      "                 mAP50         mAP50-95          \n",
      "                  mean     std     mean     std  \n",
      "Dataset                                          \n",
      "bumblebee-data  0.9807  0.0015   0.8081  0.0558  \n",
      "honeybee-data   0.8679  0.1798   0.6157  0.3195  \n",
      "hoverfly-data   0.8687  0.1725   0.4989  0.2000  \n",
      "ivybee-data     0.7316  0.3648   0.6065  0.3376  \n",
      "\n",
      "3. MODEL-DATASET COMBINATIONS:\n",
      "                           Precision  Recall  F1-Score   mAP50  mAP50-95\n",
      "Model      Dataset                                                      \n",
      "beeYOLO    bumblebee-data     0.9798  0.9700    0.9797  0.9797    0.7687\n",
      "           honeybee-data      0.7617  0.6662    0.7408  0.7408    0.3897\n",
      "           hoverfly-data      0.8333  0.6711    0.7467  0.7467    0.3575\n",
      "           ivybee-data        0.5000  0.4474    0.4736  0.4736    0.3678\n",
      "insectYOLO bumblebee-data     0.9796  0.9800    0.9818  0.9818    0.8476\n",
      "           honeybee-data      1.0000  1.0000    0.9950  0.9950    0.8416\n",
      "           hoverfly-data      1.0000  0.9868    0.9907  0.9907    0.6403\n",
      "           ivybee-data        0.9868  0.9868    0.9895  0.9895    0.8452\n"
     ]
    }
   ],
   "source": [
    "def analyze_model_performance(results_df):\n",
    "    \"\"\"Analyze performance across different models and datasets.\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"MODEL PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Overall performance by model\n",
    "    print(\"\\n1. OVERALL PERFORMANCE BY MODEL:\")\n",
    "    model_performance = results_df.groupby('Model').agg({\n",
    "        'Precision': ['mean', 'std', 'count'],\n",
    "        'Recall': ['mean', 'std'],\n",
    "        'F1-Score': ['mean', 'std'],\n",
    "        'mAP50': ['mean', 'std'],\n",
    "        'mAP50-95': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    print(model_performance)\n",
    "    \n",
    "    # Performance by dataset\n",
    "    print(\"\\n2. PERFORMANCE BY DATASET:\")\n",
    "    dataset_performance = results_df.groupby('Dataset').agg({\n",
    "        'Precision': ['mean', 'std', 'count'],\n",
    "        'Recall': ['mean', 'std'],\n",
    "        'F1-Score': ['mean', 'std'],\n",
    "        'mAP50': ['mean', 'std'],\n",
    "        'mAP50-95': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    print(dataset_performance)\n",
    "    \n",
    "    # Model-Dataset combinations\n",
    "    print(\"\\n3. MODEL-DATASET COMBINATIONS:\")\n",
    "    combo_performance = results_df.groupby(['Model', 'Dataset']).agg({\n",
    "        'Precision': 'mean',\n",
    "        'Recall': 'mean',\n",
    "        'F1-Score': 'mean',\n",
    "        'mAP50': 'mean',\n",
    "        'mAP50-95': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    print(combo_performance)\n",
    "    \n",
    "    return model_performance, dataset_performance, combo_performance\n",
    "\n",
    "# Run the analysis\n",
    "model_perf, dataset_perf, combo_perf = analyze_model_performance(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ec19a1",
   "metadata": {},
   "source": [
    "# Visualization: Performance Barplots\n",
    "\n",
    "Now create barplots for each metric, grouped by model with datasets on the x-axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbf8fd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating performance visualizations...\n",
      "âœ“ Combined performance plot saved to: ../benchmarks/comparisons/performance_comparison_all_metrics.png\n",
      "âœ“ Precision plot saved to: ../benchmarks/comparisons/performance_precision.png\n",
      "âœ“ Recall plot saved to: ../benchmarks/comparisons/performance_recall.png\n",
      "âœ“ F1-Score plot saved to: ../benchmarks/comparisons/performance_f1_score.png\n",
      "âœ“ mAP50 plot saved to: ../benchmarks/comparisons/performance_map50.png\n",
      "âœ“ mAP50-95 plot saved to: ../benchmarks/comparisons/performance_map50_95.png\n",
      "âœ“ Performance heatmap saved to: ../benchmarks/comparisons/performance_heatmap_map50_95.png\n",
      "\n",
      "âœ“ All visualization files saved to: ../benchmarks/comparisons\n",
      "âœ“ Visualizations completed! Check: ../benchmarks/comparisons\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "def create_performance_barplots(results_df, output_dir=\"../benchmarks/comparisons\"):\n",
    "    \"\"\"Create barplots for each performance metric, grouped by model.\"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Define metrics to plot\n",
    "    metrics = ['Precision', 'Recall', 'F1-Score', 'mAP50', 'mAP50-95']\n",
    "    \n",
    "    # Create a figure with subplots for each metric\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Model Performance Comparison Across Datasets', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Flatten axes for easier iteration\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            \n",
    "            # Create pivot table for the current metric\n",
    "            pivot_data = results_df.pivot(index='Dataset', columns='Model', values=metric)\n",
    "            \n",
    "            # Create grouped bar plot\n",
    "            pivot_data.plot(kind='bar', ax=ax, width=0.8)\n",
    "            \n",
    "            ax.set_title(f'{metric}', fontweight='bold')\n",
    "            ax.set_xlabel('Dataset')\n",
    "            ax.set_ylabel(metric)\n",
    "            ax.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for container in ax.containers:\n",
    "                ax.bar_label(container, fmt='%.3f', fontsize=8)\n",
    "    \n",
    "    # Remove the extra subplot if we have 5 metrics\n",
    "    if len(metrics) < 6:\n",
    "        axes[-1].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    \n",
    "    # Save the combined plot\n",
    "    combined_plot_path = output_path / \"performance_comparison_all_metrics.png\"\n",
    "    plt.savefig(combined_plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"âœ“ Combined performance plot saved to: {combined_plot_path}\")\n",
    "    \n",
    "    # Create individual plots for each metric\n",
    "    for metric in metrics:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Create pivot table for the current metric\n",
    "        pivot_data = results_df.pivot(index='Dataset', columns='Model', values=metric)\n",
    "        \n",
    "        # Create grouped bar plot\n",
    "        ax = pivot_data.plot(kind='bar', width=0.8)\n",
    "        \n",
    "        plt.title(f'{metric} Comparison Across Models and Datasets', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Dataset', fontsize=12)\n",
    "        plt.ylabel(metric, fontsize=12)\n",
    "        plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for container in ax.containers:\n",
    "            ax.bar_label(container, fmt='%.3f', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save individual plot\n",
    "        individual_plot_path = output_path / f\"performance_{metric.lower().replace('-', '_')}.png\"\n",
    "        plt.savefig(individual_plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"âœ“ {metric} plot saved to: {individual_plot_path}\")\n",
    "        \n",
    "        plt.close()\n",
    "    \n",
    "    # Create a summary heatmap\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Create pivot table for mAP50-95 (most important metric)\n",
    "    heatmap_data = results_df.pivot(index='Dataset', columns='Model', values='mAP50-95')\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "                center=0.5, vmin=0, vmax=1, cbar_kws={'label': 'mAP50-95'})\n",
    "    \n",
    "    plt.title('mAP50-95 Performance Heatmap', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Model', fontsize=12)\n",
    "    plt.ylabel('Dataset', fontsize=12)\n",
    "    \n",
    "    # Save heatmap\n",
    "    heatmap_path = output_path / \"performance_heatmap_map50_95.png\"\n",
    "    plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"âœ“ Performance heatmap saved to: {heatmap_path}\")\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\nâœ“ All visualization files saved to: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "# Create the performance barplots\n",
    "print(\"\\nCreating performance visualizations...\")\n",
    "visualization_dir = create_performance_barplots(results_df)\n",
    "print(f\"âœ“ Visualizations completed! Check: {visualization_dir}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
